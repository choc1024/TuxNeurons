import random
import string
import math

####################################################################################################
########## SECTION 1 ###############################################################################
####### PUBLIC FUNCTIONS ###########################################################################
####################################################################################################





"""
Register: Function

Registers and creates a new neuron using the Neuron class
"""
def register(name, registry, cglobals, type, bias=None, limit=None):
    iden = ''.join(random.choices(string.ascii_letters + string.digits, k=10))
    if type == "s":
        cglobals[name] = sNeuron(iden)
    elif type == "n":
        cglobals[name] = Neuron(iden, bias)
    elif type == "b":
        cglobals[name] = bNeuron(iden, limit, bias)
    elif type == "e":
        cglobals[name] = eNeuron(iden, bias)
    else:
        raise ValueError("Must be s, n, b or e")
    registry[iden] = name


"""
Network: Function

Creates a new network using the Neuron class.
I suck at loops so some (most) of the code is generated by Tabnine Autocomplete.
Thank god that autocompletion is a thing.
I've tested this and it works.
"""
def network(registry, cglobals, sn, hl, hn, en):
    """
    Network: Function

    Creates a new network using the Neuron class

    sn: start neurons
    hl: hidden layers
    hn: hidden neurons
    en: end neurons
    registry: the registry of the network
    cglobals: the global variables of the network
    """
    
    # Creates the network
    tn = sn + (hl*hn) + en
    for i in range(sn):
        register(f"s{i}", registry, cglobals, "s", 0)
    for i in range(hl):
        for j in range(hn):
            register(f"n{i}{j}", registry, cglobals, "n", 0)
    for i in range(en):
        register(f"e{i}", registry, cglobals, "e", 0)
    
    # Binds the neurons
    for i in range(sn):
        for j in range(hn):
            cglobals[f"s{i}"].bind(cglobals[f"n0{j}"], 1)
    for i in range(hl-1):
        for j in range(hn):
            for k in range(en):
                cglobals[f"n{i}{j}"].bind(cglobals[f"n{i+1}{k}"], 1)
    # Bind the neurons from the second to last layer to the end neurons
    for i in range(hn):
        for j in range(en):
            cglobals[f"n{hl-1}{i}"].bind(cglobals[f"e{j}"], 1)
        


"""
getByID: Function

Gets the name of a variable neuron using its ID
"""
def getByID(id, registry):
    obj = registry[id]
    return obj










####################################################################################################
########## SECTION 2 ###############################################################################
######## Neuron Classes ############################################################################
####################################################################################################





"""
Neuron: Class

Base for the entire library. It makes a neuron.
"""
class Neuron:
    def __init__(self, iden, bias=None):
        # Set a random bias if none is provided
        self.bias = bias if bias is not None else random.uniform(-5, 5)
        # Generate a random 10-character alphanumeric ID
        self.iden = iden
        # Placeholder for inputs
        self.inputs = []

        self.connections = {}
        
        self.parents = 0

    def bind(self, neuron, weight):
        self.connections[neuron.iden] = weight
        neuron.parents += 1
        

    def input(self, value, registry, cglobals):
        """Accepts an input value (e.g., from another neuron)."""
        self.inputs.append(value)
        if len(self.inputs) == self.parents:
            value = relu(sum(self.inputs) + self.bias)

            for i in range(len(self.connections)):
                keys = list(self.connections.keys())
                values = list(self.connections.values())
                outN = getByID(keys[i], registry)
                cglobals[outN].input(float(values[i])*value, registry, cglobals)

    def __repr__(self):
        return f"Neuron(ID={self.iden}, Bias={self.bias})"

    def b(self, bias):
        self.bias = bias

    def w(self, weight, iden):
        self.connections[iden] = weight



"""
sNeuron: Class

Based on the main Neuron class, it makes a neuron that is more
adapted for being a start neuron, so where the entire network begins.
"""
class sNeuron:
    def __init__(self, iden):
        # Generate a random 10-character alphanumeric ID
        self.iden = iden
        # Placeholder for inputs
        self.inputs = []

        self.connections = {}

    def bind(self, neuron, weight):
        self.connections[neuron.iden] = weight
        neuron.parents += 1

    def input(self, value, registry, cglobals):
        """Accepts an input value (e.g., from another neuron)."""
        for i in range(len(self.connections)):
            keys = list(self.connections.keys())
            values = list(self.connections.values())

            outN = getByID(keys[i], registry)
            cglobals[outN].input(float(values[i])*value, registry, cglobals)


    def __repr__(self):
        return f"Neuron(ID={self.iden}, Bias=N/A)"

    def w(self, weight, iden):
        self.connections[iden] = weight



"""
bNeuron: Class

Buffer Neurons take multiple inputs until it reaches the limit at which
point it will add them up, add the bias, and input it to the next neurons.

This feature has been implemented into the default neuron, but it is here just 
in case anyone needs it. 
"""
class bNeuron:
    def __init__(self, iden, limit, bias=None):
        # Set a random bias if none is provided
        self.bias = bias if bias is not None else random.uniform(-5, 5)
        # Generate a random 10-character alphanumeric ID
        self.iden = iden
        # Placeholder for inputs
        self.inputs = []

        self.connections = {}

        self.limit = limit

        self.storage = []

    def bind(self, neuron, weight):
        self.connections[neuron.iden] = weight

    def input(self, value, registry, cglobals):
        self.storage.append(value)

        """Accepts an input value (e.g., from another neuron)."""
        if len(self.storage) >= self.limit:
            value = 0
            for i in range(len(self.storage)):
                value += self.storage[i]
            value += self.bias
            self.storage = []
            for i in range(len(self.connections)):
                keys = list(self.connections.keys())
                values = list(self.connections.values())

                outN = getByID(keys[i], registry)
                cglobals[outN].input(float(values[i])*value, registry, cglobals)



    def __repr__(self):
        return f"Neuron(ID={self.iden}, Bias={self.bias})"

    def b(self, bias):
        self.bias = bias

    def w(self, weight, iden):
        self.connections[iden] = weight





"""
eNeuron: Class

The ends of the network. They take a input, add the bias and just print it.
"""
class eNeuron:
    def __init__(self, iden, bias=None):
        # Set a random bias if none is provided
        self.bias = bias if bias is not None else random.uniform(-5, 5)
        # Generate a random 10-character alphanumeric ID
        self.iden = iden
        
        self.inputs = []

        self.result = 0
        
        self.parents = 0

    def input(self, value, registry, cglobals):
        """Accepts an input value (e.g., from another neuron)."""
        self.inputs.append(value)
        if len(self.inputs) == self.parents:
            value = sig(sum(self.inputs) + self.bias)
            self.result = value

    def __repr__(self):
        return f"Neuron(ID={self.iden}, Bias={self.bias})"

    def b(self, bias):
        self.bias = bias










####################################################################################################
########## SECTION 3 ###############################################################################
########## Training ################################################################################
####################################################################################################

def sig(x):
    return 1/(1 + (math.e**(-x)))

def tanh(x):
    return (math.e**x - math.e**(-x)) / (math.e**x + math.e**(-x))

def relu(x):
    if x > 0:
        return x
    else:
        return 0
    
    
    
"""
# For those who can implement the derivative of Softmax
# (I don't have the patience to do this anymore)

def sm(x: list, ind: None):
    s = 0
    for i in range(len(x)):
        s += math.e**float(x[i])
    if ind == None:
        r = []
        for i in range(len(x)):
            r.append((math.e**float(x[i]))/s)
        return r
    else:
        return math.e**float(x[ind])/s
"""


def desig(x):
    return sig(x) * (1-sign(x))

def detanh(x):
    return 1 - tanh(x)**2

def derelu(x):
    if x > 0:
        return 1
    else:
        return 0
    


def train(registry, dataset, start, end):
    print("Testing")
