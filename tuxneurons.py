import random
import string
import math
import matplotlib.pyplot as plt

####################################################################################################
########## SECTION 1 ###############################################################################
####### PUBLIC FUNCTIONS ###########################################################################
####################################################################################################





"""
Register: Function

Registers and creates a new neuron using the Neuron class
"""
def register(name, registry, cglobals, type, bias=None, limit=None):
    iden = ''.join(random.choices(string.ascii_letters + string.digits, k=10))
    if type == "s":
        cglobals[name] = sNeuron(iden)
    elif type == "n":
        cglobals[name] = Neuron(iden, bias)
    elif type == "b":
        cglobals[name] = bNeuron(iden, limit, bias)
    elif type == "e":
        cglobals[name] = eNeuron(iden, bias)
    else:
        raise ValueError("Must be s, n, b or e")
    registry[iden] = name


"""
Network: Function

Creates a new network using the Neuron class.
I suck at loops so some (most) of the code is generated by Tabnine Autocomplete.
Thank god that autocompletion is a thing.
I've tested this and it works.
"""
def network(registry, cglobals, sn, hl, hn, en):
    """
    Network: Function

    Creates a new network using the Neuron class

    sn: start neurons
    hl: hidden layers
    hn: hidden neurons
    en: end neurons
    registry: the registry of the network
    cglobals: the global variables of the network
    """
    
    # Creates the network
    tn = sn + (hl*hn) + en
    for i in range(sn):
        register(f"s{i}", registry, cglobals, "s")
    for i in range(hl):
        for j in range(hn):
            register(f"n{i}_{j}", registry, cglobals, "n")
    for i in range(en):
        register(f"e{i}", registry, cglobals, "e")
    
    # Binds the neurons
    ## Bind neurons from start layer to the first hidden layer
    for i in range(sn):
        for j in range(hn):
            cglobals[f"s{i}"].bind(cglobals[f"n0_{j}"], 1)
    
    ## Binds hidden layer neurons
    ### For each layer
    for layer in range(hl-1):
        ### For each neuron in that layer
        for neuron in range(hn):
            ### For each connection that neuron needs to have
            for connection in range(hn):
                cglobals[f"n{layer}_{neuron}"].bind(cglobals[f"n{layer+1}_{connection}"], 1)
    # Bind the neurons from the last hidden layer to the end layer
    for neuron in range(hn):
        for end in range(en):
            cglobals[f"n{hl-1}_{neuron}"].bind(cglobals[f"e{end}"], 1)
        


"""
getByID: Function

Gets the name of a variable neuron using its ID
"""
def getByID(id, registry):
    obj = registry[id]
    return obj










####################################################################################################
########## SECTION 2 ###############################################################################
######## Neuron Classes ############################################################################
####################################################################################################





"""
Neuron: Class

Base for the entire library. It makes a neuron.
"""
class Neuron:
    def __init__(self, iden, bias=None):
        # Set a random bias if none is provided
        self.bias = 0
        #self.bias = bias if bias is not None else random.uniform(-5, 5)
        # Generate a random 10-character alphanumeric ID
        self.iden = iden
        # Placeholder for inputs
        self.inputs = []

        self.connections = {}
        
        self.parents = 0

    def bind(self, neuron, weight):
        self.connections[neuron.iden] = weight
        neuron.parents += 1
        

    def input(self, value, registry, cglobals):
        """Accepts an input value (e.g., from another neuron)."""
        self.inputs.append(value)
        if len(self.inputs) == self.parents:
            value = relu(sum(self.inputs) + self.bias)

            for i in range(len(self.connections)):
                keys = list(self.connections.keys())
                values = list(self.connections.values())
                outN = getByID(keys[i], registry)
                cglobals[outN].input(float(values[i])*value, registry, cglobals)
            self.inputs = []

    def __repr__(self):
        return f"Neuron(ID={self.iden}, Bias={self.bias})"

    def b(self, bias):
        self.bias = bias

    def w(self, weight, iden):
        self.connections[iden] = weight



"""
sNeuron: Class

Based on the main Neuron class, it makes a neuron that is more
adapted for being a start neuron, so where the entire network begins.
"""
class sNeuron:
    def __init__(self, iden):
        # Generate a random 10-character alphanumeric ID
        self.iden = iden
        # Placeholder for inputs
        self.inputs = []

        self.connections = {}
        

    def bind(self, neuron, weight):
        self.connections[neuron.iden] = weight
        neuron.parents += 1

    def input(self, value, registry, cglobals):
        """Accepts an input value (e.g., from another neuron)."""
        for i in range(len(self.connections)):
            keys = list(self.connections.keys())
            values = list(self.connections.values())
            outN = getByID(keys[i], registry)
            cglobals[outN].input(float(values[i])*float(value), registry, cglobals)


    def __repr__(self):
        return f"Neuron(ID={self.iden}, Bias=N/A)"

    def w(self, weight, iden):
        self.connections[iden] = weight



"""
bNeuron: Class

Buffer Neurons take multiple inputs until it reaches the limit at which
point it will add them up, add the bias, and input it to the next neurons.

This feature has been implemented into the default neuron, but it is here just 
in case anyone needs it. 
"""
class bNeuron:
    def __init__(self, iden, limit, bias=None):
        # Set a random bias if none is provided
        self.bias = bias if bias is not None else random.uniform(-5, 5)
        # Generate a random 10-character alphanumeric ID
        self.iden = iden
        # Placeholder for inputs
        self.inputs = []

        self.connections = {}

        self.limit = limit

        self.storage = []

    def bind(self, neuron, weight):
        self.connections[neuron.iden] = weight

    def input(self, value, registry, cglobals):
        self.storage.append(value)

        """Accepts an input value (e.g., from another neuron)."""
        if len(self.storage) >= self.limit:
            value = 0
            for i in range(len(self.storage)):
                value += self.storage[i]
            value += self.bias
            self.storage = []
            for i in range(len(self.connections)):
                keys = list(self.connections.keys())
                values = list(self.connections.values())

                outN = getByID(keys[i], registry)
                cglobals[outN].input(float(values[i])*value, registry, cglobals)



    def __repr__(self):
        return f"Neuron(ID={self.iden}, Bias={self.bias})"

    def b(self, bias):
        self.bias = bias

    def w(self, weight, iden):
        self.connections[iden] = weight





"""
eNeuron: Class

The ends of the network. They take a input, add the bias and just print it.
"""
class eNeuron:
    def __init__(self, iden, bias=None):
        # Set a random bias if none is provided
        self.bias = bias if bias is not None else random.uniform(-5, 5)
        # Generate a random 10-character alphanumeric ID
        self.iden = iden
        
        self.inputs = []

        self.result = 0
        
        self.parents = 0

    def input(self, value, registry, cglobals):
        """Accepts an input value (e.g., from another neuron)."""
        self.inputs.append(value)
        if len(self.inputs) == self.parents:
            #value = sig(sum(self.inputs) + self.bias)
            value = sum(self.inputs) + self.bias
            self.result = value
            self.inputs = []

    def __repr__(self):
        return f"Neuron(ID={self.iden}, Bias={self.bias})"

    def b(self, bias):
        self.bias = bias










####################################################################################################
########## SECTION 3 ###############################################################################
########## Training ################################################################################
####################################################################################################





'''
The format for the dataset is a list of tupples, wich each tupple containing
2 lists. Example:

dataset = [
    ([0, 0], [0]),
    ([0, 1], [1]),
    ([1, 0], [1])
]

Which means each tupple is a entry of training data.
And the first list in a tupple is the list of inputs, while the second list
is a list of the expected, correct outputs.
'''





def sig(x):
    """
    The sigmoid function.

    Maps any real-valued number to a value between 0 and 1.

    The sigmoid function is defined as 1 / (1 + e^(-x)).

    Parameters:
    x (float): The input to the function.

    Returns:
    float: The output of the function.
    """
    return 1/(1 + (math.e**(-x)))




def tanh(x):
    """
    The Hyperbolic Tangent function.

    Maps any real-valued number to a value between -1 and 1.

    The hyperbolic tangent function is defined as (e^x - e^(-x)) / (e^x + e^(-x)).

    Parameters:
    x (float): The input to the function.

    Returns:
    float: The output of the function.
    """
    return (math.e**x - math.e**(-x)) / (math.e**x + math.e**(-x))





def relu(x):
    """
    The ReLU (Rectified Linear Unit) function.

    Maps any real-valued number to its positive part.

    The ReLU function is defined as max(0, x).

    Parameters:
    x (float): The input to the function.

    Returns:
    float: The output of the function.
    """
    if x > 0:
        return x
    else:
        return 0
    
    
    
    
    
"""
# For those who can implement the derivative of Softmax
# (I don't have the patience to do this anymore)

def sm(x: list, ind: None):
    s = 0
    for i in range(len(x)):
        s += math.e**float(x[i])
    if ind == None:
        r = []
        for i in range(len(x)):
            r.append((math.e**float(x[i]))/s)
        return r
    else:
        return math.e**float(x[ind])/s
"""





def desig(x):
    """
    The derivative of the Sigmoid function.
    
    Returns sig(x) * (1 - sig(x))
    """
    return sig(x) * (1-sign(x))





def detanh(x):
    """
    The derivative of the Hyperbolic Tangent function.
    
    Returns 1 - tanh(x)^2
    """
    
    return 1 - tanh(x)**2





def derelu(x):
    """
    The derivative of the Relu function.
    Returns 1 if x is greater than 0, and 0 if not.
    """
    
    if x > 0:
        return 1
    else:
        return 0





def av(l: list):
    """
    Returns the average of the elements in the list l.

    Parameters
    ----------
    l : list
        The list of numbers to average.

    Returns
    -------
    float
        The average of the elements in l.
    """
    return sum(l) / len(l)
    
def mse(targets: list, outputs: list):
    err = []
    for output in range(len(outputs)):
        err.append((targets[output] - outputs[output])**2)
    error = av(err)
    return error

def get_error(cglobals, registry, dataset, inputs, outputs):
    err_list = []
    for data in range(len(dataset)):
        results = []
        for sneuron in range(inputs):
            cglobals[f"s{sneuron}"].input(dataset[data][0][sneuron], registry, cglobals)
        for eneuron in range(outputs):
            results.append(cglobals[f"e{eneuron}"].result)
        for result in range(len(results)):
            err_list.append((dataset[data][1][result] - results[result])**2)
    return av(err_list)

def simulated_annealing(cglobals, registry, dataset, inputs, outputs, hl, hn,
                        temp=2.0, decay=0.995, tlimit=0.0001, rate=0.1):
    while temp > tlimit:
        # Adjust the input layer weights
        for sneuron in range(inputs):
            for connection in range(hn):
                error = get_error(cglobals, registry, dataset, inputs, outputs)
                original_weight = cglobals[f"s{sneuron}"].connections[cglobals[f"n0_{connection}"].iden]
                change = random.uniform(-rate, rate)
                cglobals[f"s{sneuron}"].connections[cglobals[f"n0_{connection}"].iden] += change
                new_error = get_error(cglobals, registry, dataset, inputs, outputs)
                
                # If error increases
                if new_error > error:
                    accept = math.e**((error - new_error)/temp)
                    # There is a chance that the weight will be reset
                    # or kept to escape local maximum
                    if random.random() > accept:
                        cglobals[f"s{sneuron}"].connections[cglobals[f"n0_{connection}"].iden] = original_weight-change
                        test_error = get_error(cglobals, registry, dataset, inputs, outputs)
                        if test_error > new_error:
                            cglobals[f"s{sneuron}"].connections[cglobals[f"n0_{connection}"].iden] = original_weight
                # If error decreases, the weight is kept. But as it is already set,
                # there is no need to do anything.
        # Adjust the hidden layer weights and biases
        for layer in range(hl):
            for neuron in range(hn):
                error = get_error(cglobals, registry, dataset, inputs, outputs)
                original_bias = cglobals[f"n{layer}_{neuron}"].bias
                change = random.uniform(-rate, rate)
                cglobals[f"n{layer}_{neuron}"].bias += change
                new_error = get_error(cglobals, registry, dataset, inputs, outputs)

                # If error increases
                if new_error > error:
                    accept = math.e**((error - new_error)/temp)
                    # There is a chance that the weight will be reset
                    # or kept to escape local maximum
                    if random.random() > accept:
                        cglobals[f"n{layer}_{neuron}"].bias = original_bias - change
                        test_error = get_error(cglobals, registry, dataset, inputs, outputs)
                        if test_error > new_error:
                            cglobals[f"n{layer}_{neuron}"].bias = original_bias

                # Adjust weights
                for connection in range(len(cglobals[f"n{layer}_{neuron}"].connections)):
                    error = get_error(cglobals, registry, dataset, inputs, outputs)
                    keys = list(cglobals[f"n{layer}_{neuron}"].connections.keys())
                    values = list(cglobals[f"n{layer}_{neuron}"].connections.values())
                    original_weight = values[connection]
                    change = random.uniform(-rate, rate)
                    cglobals[f"n{layer}_{neuron}"].connections[keys[connection]] += change
                    new_error = get_error(cglobals, registry, dataset, inputs, outputs)

                    # If error increases
                    if new_error > error:
                        accept = math.e**((error - new_error)/temp)
                        # There is a chance that the weight will be reset
                        # or kept to escape local maximum
                        if random.random() > accept:
                            cglobals[f"n{layer}_{neuron}"].connections[keys[connection]] = original_weight - change
                            test_error = get_error(cglobals, registry, dataset, inputs, outputs)
                            if test_error > new_error:
                                cglobals[f"n{layer}_{neuron}"].connections[keys[connection]] = original_weight
        # Adjust the output layer biases
        for eneuron in range(outputs):
            error = get_error(cglobals, registry, dataset, inputs, outputs)
            original_bias = cglobals[f"e{eneuron}"].bias
            change = random.uniform(-rate, rate)
            cglobals[f"e{eneuron}"].bias += change
            new_error = get_error(cglobals, registry, dataset, inputs, outputs)

            # If error increases
            if new_error > error:
                accept = math.e**((error - new_error)/temp)
                # There is a chance that the weight will be reset
                # or kept to escape local maximum
                if random.random() > accept:
                    cglobals[f"e{eneuron}"].bias = original_bias - change
                    test_error = get_error(cglobals, registry, dataset, inputs, outputs)
                    if test_error > new_error:
                        cglobals[f"e{eneuron}"].bias = original_bias
        print(f"Temperature: {temp}")
        temp *= decay
        
        print(get_error(cglobals, registry, dataset, inputs, outputs))
        rate *= 0.999
